{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Extract spatial \"hotspots\" of activity from voltage imaging recording\n",
    "### Steps\n",
    "\n",
    "1) Import stuff \n",
    "2) Set data with labelled blobs of significant activity \n",
    "3) Create class for analysis and get normalised + temporally summed 2D activity heatmaps\n",
    "4) Get distance between each heatmap, and threshold for significance using spatial shuffling\n",
    "5) Cluster heatmaps based on distance, get similarity matrix \n",
    "6) Perform background correction (optional): remove overall weakly correlated heatmaps which are more likely noise\n",
    "7) Get heatmaps & similarity matrices for each level of the linkage hierarchy. Plot matrices w clusters highlighted.\n",
    "8) Search through the tree and find spatially significant hotspots, then plot/save\n",
    "\n",
    "To do:\n",
    "1) Check significance calculation between clusters\n",
    "2) Improve speed of tree-search algorithm\n",
    "4) Memory map results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, sys, importlib, glob, matplotlib.pyplot as plt, seaborn as sns, pickle as pkl, itertools\n",
    "from scripts import dfFunc as f, vFunc as vF, pltFunc as pF, analyse, synFunc as sF, synAnalysis as synAn\n",
    "sns.set_style('white')\n",
    "#sys.path.append('../NetAnalysis/Ensembles/scripts')\n",
    "#import eFunctions as eF, ePltFunc as eP\n",
    "def rel(): importlib.reload(f),importlib.reload(vF),importlib.reload(pF),importlib.reload(synAn), importlib.reload(analyse), importlib.reload(sF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - 3) Specify data and create class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data with blobs of significant activity from getActivity.ipynb. Get heatmaps of activity summed over temporal frames: ie, each heatmap is of length N x M pixels, and each value corresponds to the (normalised) temporally summed intensity over the blob of activity. The activity is normalised because if 2 heatmaps have the same intensity distribution, apart from the scaled magnitude, they probably come from the same synaptic input! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = 'data/cntData_test.pkl'      # Path to data with labelled \"blobs\" of activity (getActivity.ipynb)\n",
    "synClust = synAn.synClust(Data)         # Analysis class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Get measure of distance between each heatmap, and threshold distance for significance with spatial shuffling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a distance measure between all heatmaps to use for clustering. Set a threshold for significance by randomly spatially shuffling every heatmap N times, recalculating distance between every heatmap pair, and using a cutoff based on these distribution of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel()\n",
    "PERCENTILE = 90                 # Percentile threshold for significance (input 100 - this bc it's distance...)\n",
    "Runs = 100                      # Number of shuffling runs\n",
    "batchSize = (10, 10)            # Batch sizes for GPU: reduce if running out of memory....\n",
    "METRIC = 'correlation'          # Metric for calculating similarity between heatmaps\n",
    "\n",
    "synClust.getDistance(Runs, batchSize, percentile = 100 - PERCENTILE, \\\n",
    "                     shufflePlot = False, Metric = METRIC) # 3000 episodes = 140 seconds for 1k runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-6) Cluster heatmaps based on distance and get similarity matrix. Perform a final background correction (optional?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform clustering on the heatmap distance matrix, and create a matrix of cluster hierarchies, and a similarity matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synClust.getClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background correction (optional). The rationale for this is because heatmaps that are overall weakly correlated are probably more likely to be due to background noise. This finds the distribution of mean correlations of all heatmaps that are restricted to \"non-cell\" locations. Any heatmaps that have a mean correlation less than a threshold, based on this distribution, are removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECT = True                              # Perform background correction?\n",
    "BG_PERCENTILE = 90                          # %ile for cutoff\n",
    "SPLITS = 1                                  # For GPU: increase if memory issues....\n",
    "\n",
    "print('uncorrected activity matrix shape: ', synClust.actM.shape)\n",
    "synClust.bgCorrection(Percentile = BG_PERCENTILE, Correct = CORRECT, SPLITS = SPLITS) # change SPLIT for GPU memory use\n",
    "print('corrected activity matrix shape: ', synClust.actM.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-calculate clustering etc if background correction is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synClust.getClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Get heatmaps within each cluster for each level of the linkage hierarchy. Plot ordered similarity matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot matrices corresponding to different levels of the clustering hierarchy (starting with a single cluster), with each cluster ordered by within-cluster distance. Get heatmaps corresponding to each cluster at hierarchy level.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHierarchies = 20               # Choose arbitrary upper number of cluster hierarchies (just need enough to cover most activity...)\n",
    "\n",
    "plt.ioff()\n",
    "rows = np.linspace(-(nHierarchies + 1), -1, nHierarchies + 1).astype(int)\n",
    "synClust.plotSortSim(rows, sHigh = False, save = False, Par = False)\n",
    "plt.close('all')\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a cluster hierarchy level that \"covers\" a certain proportion of the total activity (eg, 90%). This level will then be used to search for all significantly separated clusters....   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERC_COVER = 0.95\n",
    "\n",
    "try:\n",
    "      print('cluster level and percentage for > 90 perc cell coverage is',\\\n",
    "            [(K,V) for K, V in synClust.activityCover['cell'].items() if V > PERC_COVER][-1])\n",
    "except:\n",
    "      print(\"not enough hierarchies to cover {} percent of cells activity!\".format(PERC_COVER * 100))\n",
    "\n",
    "try:\n",
    "      print('cluster level and percentage for > 90 perc total coverage is',\\\n",
    "            [(K,V) for K, V in synClust.activityCover['total'].items() if V > PERC_COVER][-1])\n",
    "except:\n",
    "      print(\"not enough hierarchies to cover {} percent of total activity!\".format(PERC_COVER * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of hierarchies used to find spatially significant clusters based on the % of activity covered. Eg, if 20 levels cover say 90% of the activity, choose this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHierarchies = 20\n",
    "synClust.rowData = synClust.rowData[-nHierarchies:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - plot a few things as a sanity check:\n",
    "1) Mean heatmap of activity associated with a cluster at each level\n",
    "2) Create movies of just the \"hotspots\" corresponding to a cluster (pick how many hotspots to shorten!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "synClust.plotHmaps()                                    # Plot mean heatmap associated with clusters for EVERY level of hierachy                                         \n",
    "\n",
    "HIERARCHY = 5       # Cluster hierarchy to create exemplar videos of activity\n",
    "N_BLOBS = 10        # Number of activity episodes to display for each cluster\n",
    "SPEED = 0.5         # Speed of movie\n",
    "#synClust.getClustMovies(HIERARCHY, Speed = 0.5, Short = N_BLOBS)     # Create video\n",
    "\n",
    "plt.close('all')\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Search through cluster hierarchy & find spatially separated clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to search through cluster hierarchy and identify significantly separated spatial clusters. Save cluster labels and plot the mean heatmap of each significant cluster. The algorithm works as follows:\n",
    "\n",
    "1) Start by selecting the largest, most strongly clustered cluster. \n",
    "2) Search \"down\" by splitting the cluster into child clusters until the children are not significant. Save all significant clusters and discard any further child clusters down the hierarchy.\n",
    "3) Move onto the next largest spatially separate cluster. First test if it is separate from existing clusters. \n",
    "    * Yes: repeat step 1\n",
    "    * No: discard, return to step 2\n",
    "4) Repeat until all clusters are accounted for, down to a specified hierarchy depth.  \n",
    "\n",
    "How to determine whether to split a parent cluster into 2 child clusters:\n",
    "1) Compute the silhouette score of the child clusters\n",
    "2) Generate N surrogate Gaussian datasets of the parent\n",
    "3) Cluster N surrogate datasets into 2 separate clusters (K-means) and get N surrogate silhouette scores\n",
    "4) Check if the child silhouette score is > than the Xth percentile of the parent-based surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = True                    # Whether to normalise Gaussians when comparing 2 clusters\n",
    "THRESH = 95                     # Significant threshold for distinct clusters based on random \n",
    "LABELS = True                   # Whether to plot the cluster label number next to each heatmap\n",
    "\n",
    "synClust.getClusterSignificance(NORM, THRESH, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sF.plotMeanEnsHm(synClust.actM, synClust.rowData, labelText=True, Dim = synClust.spatialDims, \\\n",
    "#                 Save = 'norm' + str(NORM) + '_Labels_thresh' + str(THRESH) + '_', \\\n",
    "#                 clusterList = synClust.clLabels, clusterLabels = [int(tCl) for tCl in synClust.sigClusters],\\\n",
    "#                    UPSAMPLE = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Testing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a cluster and all of it's child subclusters: either on the same plot or different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER = 307                                                       # Cluster number to plot\n",
    "\n",
    "sF.plotSubClusters(CLUSTER, synClust.clTree, synClust.clLabels, synClust.actM, \\\n",
    "                   synClust.rowData, Dim = synClust.spatialDims)    # Cluster is saved in figures folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the significance of any 2 arbitrary clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SIGNIFICANCE OF ANY 2 ARBITRARY CLUSTERS\n",
    "rel()\n",
    "sig, Dots, Hms, gData, si, scores = sF.clusterSigTest((254,305), synClust.rowData, \\\n",
    "                                    synClust.actM, Dim = synClust.spatialDims, nDots = 1000, norm = True)\n",
    "\n",
    "print(\"Separation significance is {}. Silhouette score for original clusters is {}. Mean/std of \\n \\\n",
    "silhouette scores for clusters approximated by single Gaussian is {}/{}\".format(sig, si, \\\n",
    "    np.round(np.mean(scores), 3), np.round(np.std(scores), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TEST DATA\n",
    "ALPH = 0.25\n",
    "(Y, X) = synClust.spatialDims\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (3, 3 * (Y / X)))\n",
    "\n",
    "ax.scatter(np.array(Dots[0]).T[1],np.array(Dots[0]).T[0], alpha = ALPH, color = 'black')\n",
    "ax.scatter(np.array(Dots[1]).T[1],np.array(Dots[1]).T[0], alpha = ALPH, color = 'blue')\n",
    "\n",
    "#ax.scatter(np.array(Dots[2]).T[1],np.array(Dots[2]).T[0],alpha=0.1)\n",
    "#ax.scatter(gData[0].T[1], gData[0].T[0], alpha = 0.1, color = 'green')\n",
    "\n",
    "ax.set_ylim([synClust.spatialDims[0],0]), ax.set_xlim([0,synClust.spatialDims[1]])\n",
    "\n",
    "ax.legend(['Cluster 1','Cluster 2','Estimated gaussian'], fontsize = 8)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voltage_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
